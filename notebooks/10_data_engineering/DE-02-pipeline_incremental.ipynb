{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9789f7",
   "metadata": {},
   "source": [
    "# Contexto de Negocio y Marco de Trabajo\n",
    "\n",
    "## Empresa y situaci√≥n\n",
    "El flujo de pedidos crece y requiere actualizaciones frecuentes del lakehouse. Reprocesar todo a diario es costoso.\n",
    "\n",
    "## Qu√© / Por qu√© / Para qu√© / Cu√°ndo / C√≥mo\n",
    "- Qu√©: Pipeline incremental de √≥rdenes en Parquet.\n",
    "- Por qu√©: Reducir tiempo y costos de c√≥mputo vs recargas completas.\n",
    "- Para qu√©: Mantener datos frescos para anal√≠tica y monitoreo operativo.\n",
    "- Cu√°ndo: Cada 1‚Äì4 horas seg√∫n criticidad.\n",
    "- C√≥mo: Checkpoint de fecha, filtrado incremental, append y actualizaci√≥n de checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7418738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Root directory agregado al path: f:\\GitHub\\supply-chain-data-notebooks\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n del entorno\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar el directorio ra√≠z al path para imports\n",
    "root_dir = Path.cwd().parent.parent\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(root_dir))\n",
    "\n",
    "print(f\"‚úÖ Root directory agregado al path: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7950cba",
   "metadata": {},
   "source": [
    "# Contexto de Negocio y Marco de Trabajo\n",
    "\n",
    "## Empresa y situaci√≥n\n",
    "Retailer con sistema de √≥rdenes distribuido que genera archivos diarios. Se necesita un pipeline incremental eficiente para procesar solo las √≥rdenes nuevas o modificadas.\n",
    "\n",
    "## Qu√© / Por qu√© / Para qu√© / Cu√°ndo / C√≥mo\n",
    "- Qu√©: Pipeline incremental que detecta y procesa solo las √≥rdenes nuevas desde la √∫ltima ejecuci√≥n.\n",
    "- Por qu√©: Evitar reprocesar todo el hist√≥rico diariamente ahorra tiempo y recursos computacionales.\n",
    "- Para qu√©: Mantener un data warehouse actualizado con baja latencia para an√°lisis near real-time.\n",
    "- Cu√°ndo: Ejecutar cada hora o cada 4 horas seg√∫n la criticidad del negocio.\n",
    "- C√≥mo: Usar checkpoints para rastrear la √∫ltima fecha procesada y filtrar incrementalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f7fb9",
   "metadata": {},
   "source": [
    "---\n",
    "id: \"DE-02\"\n",
    "title: \"Pipeline incremental de √≥rdenes\"\n",
    "specialty: \"Data Engineering\"\n",
    "process: \"Deliver\"\n",
    "level: \"Intermediate\"\n",
    "tags: [\"etl\", \"incremental\", \"orders\", \"python\", \"parquet\"]\n",
    "estimated_time_min: 45\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6434fa",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from src.utils.paths import DATA_RAW, DATA_PROCESSED, ensure_dirs\n",
    "from src.utils.logging import get_logger\n",
    "\n",
    "ensure_dirs()\n",
    "logger = get_logger('DE-02')\n",
    "logger.info('Iniciando pipeline incremental de √≥rdenes')\n",
    "print('‚úÖ Librer√≠as cargadas y rutas preparadas')\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9c9be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entorno listo. Ra√≠z del repo: f:\\GitHub\\supply-chain-data-notebooks\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è Preparaci√≥n de entorno y rutas\n",
    "# Si esta celda tarda demasiado o se cuelga:\n",
    "# 1) Abre la paleta de comandos (Ctrl+Shift+P)\n",
    "# 2) \"Jupyter: Restart Kernel\"\n",
    "# 3) \"Run All Above/Below\" o ejecuta desde la primera celda\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detectar ra√≠z del repo (buscando pyproject.toml o carpeta src)\n",
    "_candidates = [Path.cwd(), *Path.cwd().parents]\n",
    "_repo_root = None\n",
    "for _p in _candidates:\n",
    "    if (_p / 'pyproject.toml').exists() or (_p / 'src').exists():\n",
    "        _repo_root = _p\n",
    "        break\n",
    "if _repo_root is None:\n",
    "    _repo_root = Path.cwd()\n",
    "\n",
    "if str(_repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(_repo_root))\n",
    "\n",
    "print(f\"‚úÖ Entorno listo. Ra√≠z del repo: {_repo_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5e560",
   "metadata": {},
   "source": [
    "orders = pd.read_csv(DATA_RAW / 'orders.csv')\n",
    "orders['date'] = pd.to_datetime(orders['date'])\n",
    "# Filtrado incremental robusto usando Timestamp\n",
    "new_orders = orders[orders['date'] > last_ts]\n",
    "logger.info(f'Nuevas √≥rdenes: {len(new_orders)}')\n",
    "print(new_orders.head())\n",
    "Procesar toda la historia diariamente es costoso. Un pipeline incremental reduce tiempo y recursos.\n",
    "\n",
    "### ¬øPara qu√©?\n",
    "- Mantener actualizado el lakehouse sin recargas completas\n",
    "- Habilitar near real-time analytics\n",
    "- Optimizar uso de infraestructura\n",
    "\n",
    "### ¬øCu√°ndo?\n",
    "Ejecutar cada hora o cada 4 horas seg√∫n criticidad del negocio.\n",
    "\n",
    "### ¬øC√≥mo?\n",
    "1. Leer √∫ltima fecha procesada desde checkpoint\n",
    "2. Filtrar √≥rdenes con `date > last_processed`\n",
    "3. Append a tabla existente\n",
    "4. Actualizar checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de2d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 16:22:49,422 - DE-02 - INFO - Iniciando pipeline incremental de √≥rdenes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as cargadas y rutas preparadas\n"
     ]
    }
   ],
   "source": [
    "# üìö CONCEPTO: Pipeline incremental vs carga completa (full load)\n",
    "# CARGA COMPLETA (Full Load):\n",
    "# - Lee TODOS los datos hist√≥ricos cada ejecuci√≥n\n",
    "# - Simple de implementar (un solo SELECT * FROM source)\n",
    "# - Costoso computacionalmente: O(N) donde N = registros totales\n",
    "# - Escala mal: si tienes 100M registros, procesar 100M cada d√≠a\n",
    "\n",
    "# CARGA INCREMENTAL (Incremental Load):\n",
    "# - Lee SOLO datos nuevos/modificados desde √∫ltima ejecuci√≥n\n",
    "# - Requiere checkpoint (marca de agua, watermark, last processed timestamp)\n",
    "# - Eficiente: O(Œî) donde Œî = registros nuevos (t√≠picamente <1% del total)\n",
    "# - Escala linealmente con volumen de cambios, no con volumen total\n",
    "\n",
    "# üí° INTERPRETACI√ìN: ¬øCu√°ndo usar incremental?\n",
    "# USAR INCREMENTAL cuando:\n",
    "# - Datos hist√≥ricos grandes (>1M registros)\n",
    "# - Frecuencia de carga alta (cada hora, cada 15 min)\n",
    "# - Modificaciones raras (append-only o updates poco frecuentes)\n",
    "# - Costo de c√≥mputo es restricci√≥n (ej: Spark clusters caros)\n",
    "\n",
    "# NO usar incremental cuando:\n",
    "# - Datos peque√±os (<100K registros) ‚Üí full load es suficiente\n",
    "# - L√≥gica compleja de updates/deletes ‚Üí considerar CDC (Change Data Capture)\n",
    "# - Requerimientos de SCD Type 2 ‚Üí necesitas comparar estados completos\n",
    "\n",
    "# üîç T√âCNICA: Checkpoint persistente\n",
    "# El checkpoint debe ser:\n",
    "# - Persistente: guardado en disco/DB (no en memoria, se pierde al reiniciar)\n",
    "# - At√≥mico: actualizado SOLO si carga fue exitosa (transaccionalidad)\n",
    "# - Tipos comunes:\n",
    "#   * Timestamp: last_processed_date (asume datos ordenados por fecha)\n",
    "#   * Offset: last_record_id (para bases de datos con IDs secuenciales)\n",
    "#   * Version: last_snapshot_version (para Delta Lake, Iceberg)\n",
    "\n",
    "# üéØ APLICACI√ìN: Idempotencia en pipelines incrementales\n",
    "# Un pipeline idempotente puede ejecutarse m√∫ltiples veces con mismo resultado.\n",
    "# Para lograr idempotencia:\n",
    "# - Deduplicar registros (usar DISTINCT o GROUP BY por clave primaria)\n",
    "# - Usar UPSERT en lugar de INSERT (actualizar si existe, insertar si no)\n",
    "# - Validar checkpoint antes de actualizar (evitar brechas en datos)\n",
    "\n",
    "# ‚ö†Ô∏è SUPUESTO: Datos ordenados cronol√≥gicamente\n",
    "# Este c√≥digo asume que filtrar por date > last_checkpoint captura TODOS\n",
    "# los datos nuevos. Puede fallar si:\n",
    "# - Datos llegan desordenados (late-arriving data)\n",
    "# - Retraso en sistema source (orden 10/01 llega despu√©s que orden 10/02)\n",
    "# Soluci√≥n: usar ventana de lookback (ej: last_checkpoint - 24h) o CDC\n",
    "\n",
    "import pandas as pd\n",
    "from src.utils.paths import DATA_RAW, DATA_PROCESSED, ensure_dirs\n",
    "from src.utils.logging import get_logger\n",
    "\n",
    "ensure_dirs()\n",
    "logger = get_logger('DE-02')\n",
    "logger.info('Iniciando pipeline incremental de √≥rdenes')\n",
    "print('‚úÖ Librer√≠as cargadas y rutas preparadas')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

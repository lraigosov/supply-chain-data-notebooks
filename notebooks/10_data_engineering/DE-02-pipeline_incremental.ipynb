{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9789f7",
   "metadata": {},
   "source": [
    "# Contexto de Negocio y Marco de Trabajo\n",
    "\n",
    "## Empresa y situación\n",
    "El flujo de pedidos crece y requiere actualizaciones frecuentes del lakehouse. Reprocesar todo a diario es costoso.\n",
    "\n",
    "## Qué / Por qué / Para qué / Cuándo / Cómo\n",
    "- Qué: Pipeline incremental de órdenes en Parquet.\n",
    "- Por qué: Reducir tiempo y costos de cómputo vs recargas completas.\n",
    "- Para qué: Mantener datos frescos para analítica y monitoreo operativo.\n",
    "- Cuándo: Cada 1–4 horas según criticidad.\n",
    "- Cómo: Checkpoint de fecha, filtrado incremental, append y actualización de checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del entorno\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar el directorio raíz al path para imports\n",
    "root_dir = Path.cwd().parent.parent\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(root_dir))\n",
    "\n",
    "print(f\"✅ Root directory agregado al path: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7950cba",
   "metadata": {},
   "source": [
    "# Contexto de Negocio y Marco de Trabajo\n",
    "\n",
    "## Empresa y situación\n",
    "Retailer con sistema de órdenes distribuido que genera archivos diarios. Se necesita un pipeline incremental eficiente para procesar solo las órdenes nuevas o modificadas.\n",
    "\n",
    "## Qué / Por qué / Para qué / Cuándo / Cómo\n",
    "- Qué: Pipeline incremental que detecta y procesa solo las órdenes nuevas desde la última ejecución.\n",
    "- Por qué: Evitar reprocesar todo el histórico diariamente ahorra tiempo y recursos computacionales.\n",
    "- Para qué: Mantener un data warehouse actualizado con baja latencia para análisis near real-time.\n",
    "- Cuándo: Ejecutar cada hora o cada 4 horas según la criticidad del negocio.\n",
    "- Cómo: Usar checkpoints para rastrear la última fecha procesada y filtrar incrementalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f7fb9",
   "metadata": {},
   "source": [
    "---\n",
    "id: \"DE-02\"\n",
    "title: \"Pipeline incremental de órdenes\"\n",
    "specialty: \"Data Engineering\"\n",
    "process: \"Deliver\"\n",
    "level: \"Intermediate\"\n",
    "tags: [\"etl\", \"incremental\", \"orders\", \"python\", \"parquet\"]\n",
    "estimated_time_min: 45\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6434fa",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from src.utils.paths import DATA_RAW, DATA_PROCESSED, ensure_dirs\n",
    "from src.utils.logging import get_logger\n",
    "\n",
    "ensure_dirs()\n",
    "logger = get_logger('DE-02')\n",
    "logger.info('Iniciando pipeline incremental de órdenes')\n",
    "print('✅ Librerías cargadas y rutas preparadas')\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9c9be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Entorno listo. Raíz del repo: f:\\GitHub\\supply-chain-data-notebooks\n"
     ]
    }
   ],
   "source": [
    "# ⚙️ Preparación de entorno y rutas\n",
    "# Si esta celda tarda demasiado o se cuelga:\n",
    "# 1) Abre la paleta de comandos (Ctrl+Shift+P)\n",
    "# 2) \"Jupyter: Restart Kernel\"\n",
    "# 3) \"Run All Above/Below\" o ejecuta desde la primera celda\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detectar raíz del repo (buscando pyproject.toml o carpeta src)\n",
    "_candidates = [Path.cwd(), *Path.cwd().parents]\n",
    "_repo_root = None\n",
    "for _p in _candidates:\n",
    "    if (_p / 'pyproject.toml').exists() or (_p / 'src').exists():\n",
    "        _repo_root = _p\n",
    "        break\n",
    "if _repo_root is None:\n",
    "    _repo_root = Path.cwd()\n",
    "\n",
    "if str(_repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(_repo_root))\n",
    "\n",
    "print(f\"✅ Entorno listo. Raíz del repo: {_repo_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5e560",
   "metadata": {},
   "source": [
    "orders = pd.read_csv(DATA_RAW / 'orders.csv')\n",
    "orders['date'] = pd.to_datetime(orders['date'])\n",
    "# Filtrado incremental robusto usando Timestamp\n",
    "new_orders = orders[orders['date'] > last_ts]\n",
    "logger.info(f'Nuevas órdenes: {len(new_orders)}')\n",
    "print(new_orders.head())\n",
    "Procesar toda la historia diariamente es costoso. Un pipeline incremental reduce tiempo y recursos.\n",
    "\n",
    "### ¿Para qué?\n",
    "- Mantener actualizado el lakehouse sin recargas completas\n",
    "- Habilitar near real-time analytics\n",
    "- Optimizar uso de infraestructura\n",
    "\n",
    "### ¿Cuándo?\n",
    "Ejecutar cada hora o cada 4 horas según criticidad del negocio.\n",
    "\n",
    "### ¿Cómo?\n",
    "1. Leer última fecha procesada desde checkpoint\n",
    "2. Filtrar órdenes con `date > last_processed`\n",
    "3. Append a tabla existente\n",
    "4. Actualizar checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils.paths import DATA_RAW, DATA_PROCESSED, ensure_dirs\n",
    "from src.utils.logging import get_logger\n",
    "\n",
    "ensure_dirs()\n",
    "logger = get_logger('DE-02')\n",
    "logger.info('Iniciando pipeline incremental de órdenes')\n",
    "print('✅ Librerías cargadas y rutas preparadas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
